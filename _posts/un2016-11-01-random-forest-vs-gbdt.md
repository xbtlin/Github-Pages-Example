---
layout:     post
title:      随机森林
category: blog
description: ..
---

决策树的优缺点     

优点|缺点
--|--
训练时间复杂度低|容易过拟合，虽然剪枝可以减少，但还是不够，而且剪枝慢
可解释性强|。。

模型组合+决策树主要有两种基本形式：随机森林与GBDT。其他比较新的模型组合+决策树的算法都是来自这两个算法的延伸。


随机森林的好处     

- 它能够处理很高维度（feature很多）的数据，并且不用做特征选择
- 在训练完后，它能够给出哪些feature比较重要
- 训练速度快
- 容易做成并行化方法    

在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。


Boost是"提升"的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。

而Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的简历是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。




众数指一组数据中出现次数最多的数据值。    
例子：{苹果，苹果，香蕉，橙，橙，橙，桃}的众数是橙。    

随机森林的建立    

首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。    


完全分裂     

随机森林的优点：     
（1）比较适合做多分类问题，训练和预测速度快，在数据集上表现良好；
（2）对训练数据的容错能力强，是一种有效地估计缺失数据的一种方法，当数据集中有大比例的数据缺失时仍然可以保持精度不变和能够有效地处理大的数据集；
（3）能够处理很高维度的数据，并且不用做特征选择，即：可以处理没有删减的成千上万的变量；
（4）能够在分类的过程中可以生成一个泛化误差的内部无偏估计；
（5）能够在训练过程中检测到特征之间的相互影响以及特征的重要性程度；
（6）不会出现过度拟合；
（7）实现简单并且容易实现并行化。

为什么用gbdt?     
对缺失数据的处理较好。    
不容易过拟合。     


随机森林|GBDT
--|--
树之间独立训练|训练一棵树是为了拟合前一棵树的残差
可以并行|无法并行
一般要很深，大于7层|在浅层树上表现良好（5-15层）
速度一般|速度更快
采样低效|不需要采样
训练的时候快，但预测时慢|预测时更快
精确性稍低|精确性高
